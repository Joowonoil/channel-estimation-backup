# ê¸°ìˆ  ê°€ì´ë“œ - DNN ì±„ë„ ì¶”ì • ì‹œìŠ¤í…œ

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### í”„ë¡œì íŠ¸ êµ¬ì¡°
```
DNN_channel_estimation_training/
â”œâ”€â”€ ì‹¤í–‰ íŒŒì¼
â”‚   â”œâ”€â”€ engine_v3.py           # v3 Adapter ë² ì´ìŠ¤ ëª¨ë¸
â”‚   â”œâ”€â”€ engine_v4.py           # v4 LoRA ë² ì´ìŠ¤ ëª¨ë¸
â”‚   â”œâ”€â”€ Transfer_v3_*.py       # Adapter ì „ì´í•™ìŠµ
â”‚   â””â”€â”€ Transfer_v4_*.py       # LoRA ì „ì´í•™ìŠµ
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ adapter.py             # Adapter ëª¨ë“ˆ êµ¬í˜„
â”‚   â”œâ”€â”€ estimator_v3.py        # v3 ì±„ë„ ì¶”ì •ê¸°
â”‚   â”œâ”€â”€ estimator_v4.py        # v4 ì±„ë„ ì¶”ì •ê¸°
â”‚   â”œâ”€â”€ transformer_v3.py      # v3 Transformer
â”‚   â””â”€â”€ transformer_v4.py      # v4 Transformer
â””â”€â”€ config/
    â”œâ”€â”€ config_v3.yaml         # v3 ë² ì´ìŠ¤ ì„¤ì •
    â”œâ”€â”€ config_v4.yaml         # v4 ë² ì´ìŠ¤ ì„¤ì •
    â””â”€â”€ config_transfer_*.yaml # ì „ì´í•™ìŠµ ì„¤ì •
```

## ğŸ§  ëª¨ë¸ ì•„í‚¤í…ì²˜

### ê³µí†µ êµ¬ì¡°
- **ì…ë ¥**: (batch, 14_symbols, 3072_subcarriers, 2_channels)
- **Transformer**: 4 layers, 8 heads, d_model=128
- **FFN**: dim_feedforward=1024
- **ì¶œë ¥**: ì¶”ì • ì±„ë„ ì‘ë‹µ + ë³´ìƒëœ ìˆ˜ì‹  ì‹ í˜¸

### v3 Adapter ì•„í‚¤í…ì²˜
```python
class Adapter(nn.Module):
    """ë³‘ë ¬ Adapter ëª¨ë“ˆ - ê° Transformer ë ˆì´ì–´ì— ì¶”ê°€"""
    def __init__(self, input_dim=128, bottleneck_dim=64, dropout=0.1):
        self.fc1 = nn.Linear(input_dim, bottleneck_dim)  # Down-projection
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
        self.fc2 = nn.Linear(bottleneck_dim, input_dim)  # Up-projection
    
    def forward(self, x):
        residual = x
        x = self.fc2(self.dropout(self.relu(self.fc1(x))))
        return residual + x  # Residual connection
```

**íŠ¹ì§•**:
- ì¶”ê°€ íŒŒë¼ë¯¸í„°: ~1.3% (131,072ê°œ)
- ëª¨ë“ˆì‹ í™•ì¥ ê°€ëŠ¥
- ì¶”ë¡  ì‹œ ì¶”ê°€ ì—°ì‚° í•„ìš”

### v4 LoRA ì•„í‚¤í…ì²˜
```python
# ë¶„ë¦¬ëœ Projection Layers (LoRA íƒ€ê²Ÿ)
self.mha_q_proj = Linear(d_model, d_model)  # Query
self.mha_k_proj = Linear(d_model, d_model)  # Key  
self.mha_v_proj = Linear(d_model, d_model)  # Value
self.out_proj = Linear(d_model, d_model)    # Output
self.ffnn_linear1 = Linear(d_model, dim_feedforward)
self.ffnn_linear2 = Linear(dim_feedforward, d_model)
```

**LoRA ë¶„í•´**:
```
W' = W + Î”W = W + BA
- B: (dÃ—r), A: (rÃ—k) where r=8
- íŒŒë¼ë¯¸í„°: rÃ—(d+k) << dÃ—k
```

**íŠ¹ì§•**:
- ì¶”ê°€ íŒŒë¼ë¯¸í„°: ~1% (98,304ê°œ)
- ë³‘í•© í›„ ì›ë³¸ê³¼ ë™ì¼í•œ ì¶”ë¡  ì†ë„
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì 

## ğŸ“Š ë°ì´í„° ì²˜ë¦¬

### ì±„ë„ ë°ì´í„° êµ¬ì¡°
```python
# DMRS ì°¸ì¡° ì‹ í˜¸ ì„¤ì •
ref_conf_dict = {
    'dmrs': [0, 3072, 6]  # [ì‹œì‘, ë, ê°„ê²©]
}

# ì±„ë„ íƒ€ì…
channel_types = [
    "InF_Los", "InF_Nlos",    # Indoor Factory
    "RMa_Los", "RMa_Nlos",    # Rural Macro
    "InH_Los", "InH_Nlos",    # Indoor Hotspot
    "UMa_Los", "UMa_Nlos",    # Urban Macro
    "UMi_Los", "UMi_Nlos"     # Urban Micro
]
```

### ë°ì´í„° ì „ì²˜ë¦¬
1. **PDP ë¡œë“œ**: `.mat` íŒŒì¼ì—ì„œ Power Delay Profile ì½ê¸°
2. **ë³µì†Œìˆ˜ ë¶„ë¦¬**: ì‹¤ìˆ˜ë¶€/í—ˆìˆ˜ë¶€ë¡œ ë¶„ë¦¬ (2ì±„ë„)
3. **ì •ê·œí™”**: ì±„ë„ë³„ ì •ê·œí™”
4. **DMRS ì¶”ì¶œ**: ì°¸ì¡° ì‹ í˜¸ ìœ„ì¹˜ì—ì„œ ìƒ˜í”Œë§

## âš¡ ìµœì í™” ê¸°ë²•

### ë©”ëª¨ë¦¬ ìµœì í™”
```python
# Mixed Precision Training
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

with autocast():
    output = model(input)
    loss = criterion(output, target)
```

### ê³„ì‚° ìµœì í™”
```python
# PyTorch 2.0 Compile
model = torch.compile(model)

# Efficient Attention
F.scaled_dot_product_attention(q, k, v)
```

### TensorRT ë³€í™˜
```python
# TensorRT ìµœì í™” (ì¶”ë¡ ìš©)
import torch_tensorrt

trt_model = torch_tensorrt.compile(
    model,
    inputs=[torch.randn(batch_size, 14, 3072, 2)],
    enabled_precisions={torch.float16}
)
```

## ğŸ”§ í•µì‹¬ í´ë˜ìŠ¤

### Engine í´ë˜ìŠ¤ êµ¬ì¡°
```python
class Engine_v4:
    def __init__(self, conf_file):
        self._conf = yaml.safe_load(conf_file)
        self._estimator = Estimator_v4(conf_file)
        self._dataset, self._dataloader = get_dataset_and_dataloader()
        self.set_optimizer()
    
    def train(self):
        for data in self._dataloader:
            # Forward pass
            ch_est, _ = self._estimator(rx_signal)
            # NMSE loss
            loss = self.calculate_nmse_loss(ch_est, ch_true)
            # Backward pass
            loss.backward()
            self._optimizer.step()
```

### TransferLearning í´ë˜ìŠ¤
```python
class TransferLearningEngine:
    def load_model(self):
        # v3: Adapter ì¶”ê°€
        self._estimator = Estimator_v3(conf_file)
        # ë² ì´ìŠ¤ íŒŒë¼ë¯¸í„° ë™ê²°
        for name, param in self._estimator.named_parameters():
            if 'adapter' not in name:
                param.requires_grad = False
        
        # v4: LoRA ì ìš©
        from peft import get_peft_model, LoraConfig
        lora_config = LoraConfig(r=8, lora_alpha=8, ...)
        self._estimator = get_peft_model(self._estimator, lora_config)
```

## ğŸ“ˆ ì„±ëŠ¥ ë©”íŠ¸ë¦­

### NMSE (Normalized Mean Square Error)
```python
def calculate_nmse(pred, target):
    mse = torch.mean(torch.abs(pred - target) ** 2)
    norm = torch.mean(torch.abs(target) ** 2)
    nmse = mse / norm
    return 10 * torch.log10(nmse)  # dB scale
```

### í‰ê°€ ì§€í‘œ
- **ì±„ë„ ì¶”ì • ì •í™•ë„**: NMSE in dB
- **ìˆ˜ë ´ ì†ë„**: iterations to convergence
- **ì¶”ë¡  ì‹œê°„**: ms per batch
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: GB (training/inference)