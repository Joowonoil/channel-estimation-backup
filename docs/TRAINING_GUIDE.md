# í›ˆë ¨ ê°€ì´ë“œ - ì‹¤í–‰ ë° ì„¤ì •

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### í™˜ê²½ ì„¤ì •
```bash
# Vast AI ìë™ ì„¤ì¹˜
curl -sSL https://raw.githubusercontent.com/joowonoil/channel-estimation/main/setup_vast_ai.sh | bash

# Docker í™˜ê²½
docker pull joowonoil/channel-estimation-env:latest
docker run --gpus all -it joowonoil/channel-estimation-env:latest
```

### í•„ìˆ˜ íŒ¨í‚¤ì§€
```
pytorch==2.4.1
transformers==4.36.0
peft==0.7.0  # LoRA support
tensorrt
wandb
```

## ğŸ“‹ í›ˆë ¨ ì›Œí¬í”Œë¡œìš°

### 1ë‹¨ê³„: ë² ì´ìŠ¤ ëª¨ë¸ í›ˆë ¨

#### v3 Adapter ëª¨ë¸
```bash
# config_v3.yaml ì„¤ì •
python engine_v3.py
# ì¶œë ¥: saved_model/Large_estimator_v3_base_final.pt
```

#### v4 LoRA ëª¨ë¸
```bash
# config.yaml ì„¤ì •
python engine_v4.py
# ì¶œë ¥: saved_model/Large_estimator_v4_base.pt
```

### 2ë‹¨ê³„: ì „ì´í•™ìŠµ

#### InF í™˜ê²½
```bash
# v3 Adapter
python Transfer_v3_InF.py
# ì¶œë ¥: Large_estimator_v3_to_InF_adapter.pt

# v4 LoRA
python Transfer_v4_InF.py
# ì¶œë ¥: Large_estimator_v4_to_InF_*.pt
```

#### RMa í™˜ê²½
```bash
# v3 Adapter
python Transfer_v3_RMa.py

# v4 LoRA
python Transfer_v4_RMa.py
```

### 3ë‹¨ê³„: ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
```bash
# ëª¨ë¸ ë¹„êµ
python simple_model_test.py
python v3_adapter_comparison.py
```

## âš™ï¸ ì„¤ì • íŒŒì¼ êµ¬ì¡°

### ë² ì´ìŠ¤ ëª¨ë¸ ì„¤ì •
```yaml
# config_v3.yaml / config.yaml
dataset:
  channel_type: ["InF_Los", "InF_Nlos", ...]  # ëª¨ë“  ì±„ë„ íƒ€ì…
  batch_size: 32
  distance_range: [10.0, 500.0]  # ë„“ì€ ê±°ë¦¬ ë²”ìœ„

training:
  lr: 0.0001                      # ë² ì´ìŠ¤ í•™ìŠµë¥ 
  num_iter: 100000                # ì¶©ë¶„í•œ í›ˆë ¨
  optimizer: 'Adam'
  use_scheduler: true             # Cosine annealing
  saved_model_name: 'Large_estimator_v3_base_final'

ch_estimation:
  transformer:
    num_layers: 4
    d_model: 128
    n_head: 8
    dim_feedforward: 1024
```

### ì „ì´í•™ìŠµ ì„¤ì •
```yaml
# config_transfer_v3_InF.yaml (Adapter)
dataset:
  channel_type: ["InF_Los_50000", "InF_Nlos_50000"]
  distance_range: [40.0, 60.0]   # íŠ¹ì • í™˜ê²½ ë²”ìœ„

training:
  lr: 0.00001                     # ë‚®ì€ í•™ìŠµë¥ 
  num_iter: 200000
  pretrained_model_name: 'Large_estimator_v3_base_final'
  num_freeze_layers: 4            # ëª¨ë“  ë ˆì´ì–´ ë™ê²°

ch_estimation:
  adapter:
    bottleneck_dim: 256
    dropout: 0.1
```

```yaml
# config_transfer_v4_InF.yaml (LoRA)
ch_estimation:
  peft:
    peft_type: LORA
    r: 8                          # LoRA rank
    lora_alpha: 8                 # ìŠ¤ì¼€ì¼ë§
    target_modules: ["mha_q_proj", "mha_k_proj", "mha_v_proj"]
    lora_dropout: 0.1
```

## ğŸ”„ í›ˆë ¨ íŒŒë¼ë¯¸í„° ê°€ì´ë“œ

### í•™ìŠµë¥  ì„ íƒ
| ë‹¨ê³„ | v3 Adapter | v4 LoRA |
|------|------------|---------|
| ë² ì´ìŠ¤ í›ˆë ¨ | 0.0001 | 0.0001 |
| ì „ì´í•™ìŠµ | 0.00001 | 0.00001 |
| Fine-tuning | 0.000001 | 0.000001 |

### ë°°ì¹˜ í¬ê¸° ê¶Œì¥
| GPU Memory | Batch Size | ì„¤ëª… |
|------------|------------|------|
| 8GB | 16 | ê¸°ë³¸ í›ˆë ¨ |
| 16GB | 32 | ê¶Œì¥ ì„¤ì • |
| 24GB+ | 64 | ë¹ ë¥¸ ìˆ˜ë ´ |

### Early Stopping ì„¤ì •
```yaml
training:
  use_early_stopping: true
  patience: 500              # ê°œì„  ì—†ì´ ëŒ€ê¸°í•  ìŠ¤í…
  delta: 0.0001             # ìµœì†Œ ê°œì„ ëŸ‰
  checkpoint_path: 'saved_model/checkpoint.pt'
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### Weights & Biases ì„¤ì •
```yaml
training:
  use_wandb: true
  wandb_proj: 'DNN_channel_estimation'
```

### ë¡œê¹… í•­ëª©
- `train/ch_nmse`: ì±„ë„ ì¶”ì • ì†ì‹¤
- `train/lr`: í˜„ì¬ í•™ìŠµë¥ 
- `val/ch_nmse`: ê²€ì¦ ì†ì‹¤
- `train/grad_norm`: ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„

## ğŸ› ï¸ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### CUDA ë©”ëª¨ë¦¬ ë¶€ì¡±
```yaml
# ë°°ì¹˜ í¬ê¸° ê°ì†Œ
batch_size: 16  # 32 â†’ 16

# Gradient accumulation
accumulation_steps: 2
```

### ìˆ˜ë ´ ë¬¸ì œ
```python
# í•™ìŠµë¥  ì¡°ì •
lr: 0.00005  # ì¦ê°€ ë˜ëŠ” ê°ì†Œ

# Gradient clipping
max_norm: 0.5  # 1.0 â†’ 0.5
```

### ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜
```python
# strict=Falseë¡œ ë¶€ë¶„ ë¡œë“œ
model.load_state_dict(checkpoint, strict=False)

# í‚¤ ë§¤í•‘ í™•ì¸
for key in checkpoint.keys():
    print(key)
```

## ğŸ’¡ ìµœì í™” íŒ

### 1. ë°ì´í„° ë¡œë” ìµœì í™”
```python
DataLoader(
    dataset,
    batch_size=32,
    num_workers=4,      # CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ
    pin_memory=True,    # GPU ì „ì†¡ ê°€ì†
    persistent_workers=True
)
```

### 2. Mixed Precision Training
```python
# ìë™ í˜¼í•© ì •ë°€ë„
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

with autocast():
    output = model(input)
```

### 3. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
```python
# ì£¼ê¸°ì  ì €ì¥
if iteration % save_step == 0:
    torch.save({
        'iteration': iteration,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
    }, f'checkpoint_{iteration}.pt')
```

## ğŸš€ í”„ë¡œë•ì…˜ ë°°í¬

### TensorRT ë³€í™˜
```bash
python tensorrt_conversion_v4.py
# ì¶œë ¥: *.engine íŒŒì¼
```

### ONNX ë‚´ë³´ë‚´ê¸°
```python
torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    opset_version=11,
    do_constant_folding=True
)
```

### ì¶”ë¡  ì„œë²„
```python
# FastAPI ì˜ˆì œ
from fastapi import FastAPI
app = FastAPI()

@app.post("/estimate")
async def estimate_channel(data: np.ndarray):
    with torch.no_grad():
        result = model(torch.from_numpy(data))
    return result.numpy()
```