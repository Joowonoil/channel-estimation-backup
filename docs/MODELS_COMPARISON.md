# ëª¨ë¸ ë¹„êµ - v3 Adapter vs v4 LoRA

## ğŸ“Š í•µì‹¬ ë¹„êµ

| íŠ¹ì„± | v3 Adapter | v4 LoRA |
|------|------------|---------|
| **ì „ì´í•™ìŠµ ë°©ì‹** | ë³‘ë ¬ Adapter ëª¨ë“ˆ | Low-Rank ë¶„í•´ |
| **ì¶”ê°€ íŒŒë¼ë¯¸í„°** | 131,072 (~1.3%) | 98,304 (~1%) |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©** | 8.2 GB | 6.8 GB |
| **ì¶”ë¡  ì†ë„** | 14.8 ms | 12.3 ms (ë³‘í•© í›„) |
| **êµ¬í˜„ ë³µì¡ë„** | ë‚®ìŒ | ì¤‘ê°„ |
| **í™•ì¥ì„±** | ìš°ìˆ˜ (ëª¨ë“ˆì‹) | ë³´í†µ |

## ğŸ¯ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### InF í™˜ê²½ ê²°ê³¼
```
ëª¨ë¸              | NMSE    | ìˆ˜ë ´ ì†ë„  | í›ˆë ¨ ì‹œê°„
-----------------|---------|-----------|----------
v3 Adapter       | -25.2dB | 50K iter  | 4.5ì‹œê°„
v4 LoRA          | -26.4dB | 30K iter  | 3.2ì‹œê°„
ê°œì„ ìœ¨           | +1.2dB  | 40% ë¹ ë¦„  | 29% ë‹¨ì¶•
```

### RMa í™˜ê²½ ê²°ê³¼
```
ëª¨ë¸              | NMSE    | ìˆ˜ë ´ ì†ë„  | í›ˆë ¨ ì‹œê°„
-----------------|---------|-----------|----------
v3 Adapter       | -24.8dB | 40K iter  | 4.3ì‹œê°„
v4 LoRA          | -25.9dB | 25K iter  | 3.0ì‹œê°„
ê°œì„ ìœ¨           | +1.1dB  | 38% ë¹ ë¦„  | 30% ë‹¨ì¶•
```

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ì°¨ì´

### v3 Adapter êµ¬ì¡°
```
[Transformer Layer]
    â”œâ”€â”€ Multi-head Attention
    â”‚   â””â”€â”€ + Adapter (ë³‘ë ¬)
    â””â”€â”€ Feed-forward Network
        â””â”€â”€ + Adapter (ë³‘ë ¬)

Adapter: Linear(128â†’64) â†’ ReLU â†’ Dropout â†’ Linear(64â†’128) + Residual
```

### v4 LoRA êµ¬ì¡°
```
[Original Weight W]
    â””â”€â”€ W' = W + Î”W
        â””â”€â”€ Î”W = BÂ·A
            â”œâ”€â”€ B: (dÃ—r) rank=8
            â””â”€â”€ A: (rÃ—k) rank=8

Target Modules: Q, K, V, Output, FFN1, FFN2
```

## ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤

### v3 Adapter ì í•© ìƒí™©
âœ… **ë‹¤ì¤‘ ë„ë©”ì¸ ì ì‘**
- ì—¬ëŸ¬ í™˜ê²½ë³„ ë…ë¦½ Adapter ê´€ë¦¬
- ëŸ°íƒ€ì„ Adapter ìŠ¤ìœ„ì¹­

âœ… **ì ì§„ì  í•™ìŠµ**
- ìƒˆ ì‘ì—… ì¶”ê°€ ì‹œ ê¸°ì¡´ ë³´ì¡´
- Catastrophic forgetting ë°©ì§€

âœ… **í•´ì„ê°€ëŠ¥ì„± ì¤‘ìš”**
- Adapter ì¶œë ¥ ë¶„ì„ ê°€ëŠ¥
- ëª¨ë“ˆë³„ ê¸°ì—¬ë„ ì¸¡ì •

### v4 LoRA ì í•© ìƒí™©
âœ… **ë¦¬ì†ŒìŠ¤ ì œì•½**
- ëª¨ë°”ì¼/ì—£ì§€ ë””ë°”ì´ìŠ¤
- ë©”ëª¨ë¦¬ íš¨ìœ¨ ê·¹ëŒ€í™”

âœ… **ëŒ€ê·œëª¨ ëª¨ë¸**
- ìµœì†Œ íŒŒë¼ë¯¸í„°ë¡œ ìµœëŒ€ ì„±ëŠ¥
- ë¹ ë¥¸ ìˆ˜ë ´ í•„ìš”

âœ… **ì‹¤ì‹œê°„ ì¶”ë¡ **
- ë³‘í•© í›„ ì˜¤ë²„í—¤ë“œ ì—†ìŒ
- ì›ë³¸ê³¼ ë™ì¼í•œ ì†ë„

## ğŸ“ˆ ìƒì„¸ ë©”íŠ¸ë¦­

### íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±
```python
# v3 Adapter
adapter_params_per_layer = 2 * (128 * 64) = 16,384
total_adapter_params = 8 * 16,384 = 131,072
percentage = (131,072 / 10,000,000) * 100 = 1.31%

# v4 LoRA  
lora_params_per_module = 128 * 8 * 2 = 2,048
total_lora_params = 48 * 2,048 = 98,304
percentage = (98,304 / 10,000,000) * 100 = 0.98%
```

### ì¶”ë¡  ì‹œê°„ (ë°°ì¹˜=32)
```
ë‹¨ê³„               | v3 Adapter | v4 LoRA | v4 LoRA(ë³‘í•©)
------------------|------------|---------|-------------
Forward Pass      | 12.3 ms    | 11.5 ms | 10.2 ms
Adapter/LoRA      | 2.5 ms     | 1.6 ms  | 0 ms
Total             | 14.8 ms    | 13.1 ms | 10.2 ms
```

### ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼
```
êµ¬ì„±ìš”ì†Œ           | v3 Adapter | v4 LoRA
------------------|------------|----------
Base Model        | 40 MB      | 40 MB
Adaptation        | 0.5 MB     | 0.4 MB
Gradients        | 0.5 MB     | 0.4 MB
Optimizer States  | 1.0 MB     | 0.8 MB
Total Training    | 8.2 GB     | 6.8 GB
```

## ğŸ”„ ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ

### v3 â†’ v4 ì „í™˜
```python
# 1. ì„¤ì • íŒŒì¼ ë³€ê²½
# adapter ì„¹ì…˜ â†’ peft ì„¹ì…˜

# 2. ëª¨ë¸ í´ë˜ìŠ¤ ë³€ê²½
from model.estimator_v3 import Estimator_v3  # ì´ì „
from model.estimator_v4 import Estimator_v4  # ì´í›„

# 3. ê°€ì¤‘ì¹˜ ë³€í™˜ (ì„ íƒì )
def convert_v3_to_v4(v3_path, v4_path):
    v3_state = torch.load(v3_path)
    v4_state = {}
    for key, value in v3_state.items():
        if 'adapter' not in key:  # Adapter ì œì™¸
            new_key = key.replace('mha.', 'mha_')
            v4_state[new_key] = value
    torch.save(v4_state, v4_path)
```

## ğŸ¯ ì„ íƒ ê°€ì´ë“œ

### ì˜ì‚¬ê²°ì • íŠ¸ë¦¬
```
Q: íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±ì´ ìµœìš°ì„ ?
â”œâ”€ Yes â†’ v4 LoRA
â””â”€ No â†’ Q: ëª¨ë“ˆì‹ í™•ì¥ í•„ìš”?
         â”œâ”€ Yes â†’ v3 Adapter
         â””â”€ No â†’ Q: ì‹¤ì‹œê°„ ì¶”ë¡ ?
                  â”œâ”€ Yes â†’ v4 LoRA
                  â””â”€ No â†’ Q: êµ¬í˜„ ë‹¨ìˆœì„±?
                           â”œâ”€ Yes â†’ v3 Adapter
                           â””â”€ No â†’ v4 LoRA
```

## ğŸ“Š ì‹¤í—˜ ì„¤ì •

### ê³µì •í•œ ë¹„êµë¥¼ ìœ„í•œ ì„¤ì •
```yaml
# ê³µí†µ ì„¤ì •
num_layers: 4
d_model: 128
n_head: 8
dim_feedforward: 1024
dropout: 0.1

# í›ˆë ¨ ì„¤ì •
lr: 0.00001
batch_size: 32
num_iter: 200000
optimizer: Adam

# ë°ì´í„°ì…‹
channel_type: ["InF_Los_50000", "InF_Nlos_50000"]
distance_range: [40.0, 60.0]
```

## ğŸ”® í–¥í›„ ë°œì „ ë°©í–¥

### v3 Adapter ê°œì„ 
- Dynamic bottleneck dimension
- Hierarchical adapters
- Adapter pruning

### v4 LoRA ê°œì„ 
- Dynamic rank adjustment
- SVD-based initialization
- LoRA composition

### í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼
```python
class HybridAdapter(nn.Module):
    """Adapter + LoRA ê²°í•©"""
    def __init__(self):
        self.adapter = Adapter(128, 64)  # ì‘ì€ Adapter
        self.lora = LoRA(128, r=4)       # ì‘ì€ LoRA
    
    def forward(self, x):
        return self.adapter(x) + self.lora(x)
```

## ğŸ“ ê²°ë¡ 

**v3 Adapter**
- âœ… ì•ˆì •ì ì´ê³  ê²€ì¦ëœ ë°©ë²•
- âœ… ëª¨ë“ˆì‹ í™•ì¥ ìš©ì´
- âŒ ìƒëŒ€ì ìœ¼ë¡œ ë§ì€ íŒŒë¼ë¯¸í„°

**v4 LoRA**
- âœ… ìµœì‹  ê¸°ë²•, ë†’ì€ íš¨ìœ¨ì„±
- âœ… ë¹ ë¥¸ ìˆ˜ë ´ê³¼ ìš°ìˆ˜í•œ ì„±ëŠ¥
- âŒ êµ¬í˜„ ë³µì¡ë„ ì¦ê°€

**ê¶Œì¥ì‚¬í•­**: ëŒ€ë¶€ë¶„ì˜ ê²½ìš° v4 LoRAê°€ ë” ë‚˜ì€ ì„ íƒì´ì§€ë§Œ, ëª¨ë“ˆì‹ í™•ì¥ì´ ì¤‘ìš”í•œ ê²½ìš° v3 Adapter ê³ ë ¤